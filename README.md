![Lighthouse Parade Hero Image](./assets/hero.svg)

# Lighthouse Parade

A Node.js command line tool that crawls a domain and gathers lighthouse performance data for every page.

## Why?

There are great tools for doing performance analysis on a single web page. We use [Lighthouse](https://developers.google.com/web/tools/lighthouse) and [WebPageTest](https://webpagetest.org/) for this all the time. But what if you want to evaluate the performance characteristics of an entire site? It is tedious to manually run a report for each page and then the output is a jumble of individual reports that have to be analyzed one-by-one. This tool was created to solve this problem.

## How?

With a single command, the tool will crawl an entire site, run a Lighthouse report for each page, and then output a spreadsheet with the aggregated data. There is an [example of the generated spreadsheet](./examples/exampleAggregatedMobileReport.csv) in the `examples/` directory of this project. Each row in the spreadsheet is a page on the site, and each individual performance metric is a column. This makes it very easy to perform high-level analysis because you can sort the rows by whichever metric you are analyzing. This immediately shows the best and worst performing pages. In the following example, the rows are sorted by first contentful paint.

![Lighthouse data from multiple reports aggregated into a single spreadsheet](./assets/lighthouse-data-by-lcp.png)

It is also easy to graph data in this format. The following example is a histogram showing a problematic distribution of largest contentful paint scores.

![Histogram showing LCP scores](./assets/lcp_histogram.svg)

## Installation

1. `nvm use` (if you use NVM)
2. `npm install`

## Usage

### Scan site: crawl + lighthouse + aggregate

`npm run scan -- <URL>`

Ex: `npm run scan -- http://www.dfwfreeways.com/`

Runs a crawler on the provided URL. Discovers all URLs and runs a lighthouse report on each HTML page, then writes them to a CSV file located in `./data/<timestamp>/urls.csv`. The individual reports are written to `./data/<timestamp>/reports/`. At the end, each report file is bundled into one aggregated report CSV with each row representing a URL and each column is a metric. This is a combination of each of the commands below.

### Discover site URLs

`npm run urls -- <URL>`

Ex: `npm run url -- http://www.dfwfreeways.com/`

Runs a crawler on the provided URL. Discovers all URLs and writes them to a CSV file located in `./data/<timestamp>/urls.csv` .
By default, a `robots.txt` file will be ignored, but this flag can be manually changed at the top of `urls_task.js`.

### Generate Lighthouse reports 

`npm run reports -- <path/to/urls.csv>`

Ex: `npm run reports -- data/1595551804243/urls.csv`

Generates a Lighthouse report for each URL in the provided CSV file. Non-HTML content-types will be ignored (Ex: CSS, PNG, JSON).
The default report format is CSV, but this flag can me manually changed at the top of `lighthouse_task.js`. Each report will be written
to a `reports/` directory in the same directory as the input CSV file.

### Aggregate Lighthouse reports 

`npm run combine -- <path/to/reports/dir>`

Ex: `npm run combine -- data/1595551804243/reports`

Generates a single spreadsheet with rows for each individual Lighthouse report found in the directory


## Analysis spreadsheet template

After running a scan, you can import your aggregated report data into this [Google Sheet template](https://docs.google.com/spreadsheets/d/1n2VtFjLH5PzVQ-PCGCHc03ZFO79OT7CU_2lsMc6jFUI), which includes the following features:

* Basic formatting 
* The three [web core vitals](https://web.dev/vitals/) columns are highlighted
* Histograms for each web core vital metrics are automatically generated
* Averages and median scores for the web core vitals are also calculated next to each histogram

### Instructions

To use this template, it is important to follow these instructions very carefully when importing:

1. Visit the template and choose File > Make a copy. You will now have your own writable copy.
1. Click cell A:1 in the top left corner.
1. File > Import...
1. In the following dialogue, upload the aggregatedMobileReport.csv file that was generated by the script.
1. **Under Import Location, select Replace data at selected cell**.
1. Click Import Data 

## Additional docs

The follow links have documentation related to the libraries used by this tool.

* [Lighthouse `lhr` object properties](https://github.com/GoogleChrome/lighthouse/blob/master/docs/understanding-results.md)
* [Using Lighthouse programmatically](https://github.com/GoogleChrome/lighthouse/blob/master/docs/readme.md#using-programmatically)
* [Lighthouse CLI options](https://github.com/GoogleChrome/lighthouse#using-the-node-cli)
* [Lighthouse applies network throttling by defualt...](https://github.com/GoogleChrome/lighthouse#how-does-lighthouse-use-network-throttling-and-how-can-i-make-it-better)
* [Chrome flags/switches](https://peter.sh/experiments/chromium-command-line-switches/)
* [Node CSV docs](https://csv.js.org/)

